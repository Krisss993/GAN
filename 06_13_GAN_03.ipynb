{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LybOhWk3cAeBst4ccPRxY3eAxZ5Y_R1G",
      "authorship_tag": "ABX9TyPJfbe2OLX5cBeiVujeJE4G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "edce130c214b40fa9667e8a7e149e747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05d53c432d3745b08e371a2db3b55af1",
              "IPY_MODEL_76371f7d63954d3f9988b4da973169d2",
              "IPY_MODEL_45a33fd2477c4139b04274db625d31dd"
            ],
            "layout": "IPY_MODEL_9e823c71052440ecb62dda39f490929d"
          }
        },
        "05d53c432d3745b08e371a2db3b55af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9b73971c36041a7be7487119eac35dd",
            "placeholder": "​",
            "style": "IPY_MODEL_7eee846b336a4b4090a300be69461c9b",
            "value": "  0%"
          }
        },
        "76371f7d63954d3f9988b4da973169d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35182e3f88dc43998fa40bb7d2cda8f0",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d2e1ac836bc4f6099b6fcc91df416fb",
            "value": 0
          }
        },
        "45a33fd2477c4139b04274db625d31dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8e3b2456b2499aba2158356f4f3041",
            "placeholder": "​",
            "style": "IPY_MODEL_9fd8b21e7c854e8c80189a24d41096d4",
            "value": " 0/79 [00:00&lt;?, ?it/s]"
          }
        },
        "9e823c71052440ecb62dda39f490929d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b73971c36041a7be7487119eac35dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eee846b336a4b4090a300be69461c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35182e3f88dc43998fa40bb7d2cda8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d2e1ac836bc4f6099b6fcc91df416fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d8e3b2456b2499aba2158356f4f3041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd8b21e7c854e8c80189a24d41096d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krisss993/GAN/blob/main/06_13_GAN_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function:\n",
        "Earth movel distance - The effort needed to make both distributions equal\n",
        "- Critics(discriminator) values not restricted to be between 0 and 1\n",
        "- Even for very different distributions, gradients are significant and high enough to drive the process in the right way"
      ],
      "metadata": {
        "id": "TU5HgPR-5Qpv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gA8kp8YO5C4f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import PIL\n",
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login(key='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-miigBt-O8eT",
        "outputId": "9837a379-aa83-4eee-fca9-24a51537b631"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show(tensor, num=25, wandbactivation=0, name=''):\n",
        "  data = tensor.detach().cpu()\n",
        "  grid = make_grid(data[:num], nrow=5).permute(1,2,0)\n",
        "\n",
        "  # optional\n",
        "  # wandb - online activation\n",
        "  if (wandbactivation==1 and wandbact==1):\n",
        "      wandb.log({name:wandb.Image(grid.numpy().clip(0,1))})\n",
        "\n",
        "\n",
        "  # cliping pixels to range(0,1)\n",
        "  plt.imshow(grid.clip(0,1))\n",
        "  plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "dvsaUKV3LkUe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## hyperparameters and general parameters\n",
        "\n",
        "n_epochs = 1000\n",
        "batch_size = 128\n",
        "lr = 1e-4\n",
        "\n",
        "# z_dim - input noise latent vector dim\n",
        "z_dim = 200\n",
        "device = 'cuda'\n",
        "\n",
        "cur_step = 0\n",
        "\n",
        "# 5 cycles training of the critic, then 1 of the generator\n",
        "# generally critic needs more training than the generator\n",
        "crit_cycles = 5\n",
        "\n",
        "gen_losses = []\n",
        "crit_losses = []\n",
        "show_step  = 35\n",
        "save_step = 35\n",
        "\n",
        "# optional, tracking stats online\n",
        "wandbact = 1\n"
      ],
      "metadata": {
        "id": "L06333feN6QG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional wandb\n",
        "%%capture\n",
        "# experiment_name = wandb.util.generate_id()\n",
        "experiment_name = 'MY EXP'\n",
        "myrun = wandb.init(\n",
        "    project='wgan',\n",
        "    name=experiment_name,\n",
        "    group=experiment_name,\n",
        "    config={'optimizer':'adam',\n",
        "            'model':'wgan gp',\n",
        "            'epoch':'1000',\n",
        "            'batch_size':128\n",
        "            }\n",
        ")\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "6eme3HZxPbPv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional wandb\n",
        "print(experiment_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBptDIE_P7Tu",
        "outputId": "59051fda-cebe-4f6b-c795-4703a8e07a59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MY EXP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "  # d_dim - internal dimension for the output of the convolutional layers\n",
        "  def __init__(self, z_dim=64, d_dim=16):\n",
        "    super(Generator, self).__init__()\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    self.gen = nn.Sequential(\n",
        "\n",
        "        # ConvTranspose2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "        # n - width or height\n",
        "        # (n - 1) * stride - 2 * padding + kernel_size\n",
        "\n",
        "        # generator starts 1x1 pixel and z_dim number of channels and it gives it dimensionality of latent space\n",
        "        # starting with 200 channels bringing up channels to 512 and increasing size of the image\n",
        "\n",
        "        # 1st block\n",
        "        nn.ConvTranspose2d(in_channels=z_dim, out_channels=d_dim*32, kernel_size=4, stride=1, padding=0),\n",
        "        # normalizing values for improving stability, num_features = out from conv layer\n",
        "        nn.BatchNorm2d(num_features=d_dim*32),\n",
        "        # applying nonlinearity\n",
        "        nn.ReLU(True),\n",
        "\n",
        "\n",
        "        # 2nd block\n",
        "        nn.ConvTranspose2d(in_channels=d_dim*32, out_channels=d_dim*16, kernel_size=4, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(d_dim*16),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # 3rd block\n",
        "        nn.ConvTranspose2d(in_channels=d_dim*16, out_channels=d_dim*8, kernel_size=4, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(d_dim*8),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # 4th block\n",
        "        nn.ConvTranspose2d(in_channels=d_dim*8, out_channels=d_dim*4, kernel_size=4, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(d_dim*4),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # 5th block\n",
        "        nn.ConvTranspose2d(in_channels=d_dim*4, out_channels=d_dim*2, kernel_size=4, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(d_dim*2),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        # 6th block\n",
        "        # in output layer we stay with ONLY 3 channels\n",
        "        nn.ConvTranspose2d(in_channels=d_dim*2, out_channels=d_dim*1, kernel_size=4, stride=2, padding=1),\n",
        "        # no batch norm\n",
        "\n",
        "        # result from -1 to 1\n",
        "        nn.Tanh()\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, noise):\n",
        "    # generator recives noise as input\n",
        "    x = noise.view(len(noise), self.z_dim, 1, 1) # 128(batch) x 200(z_dim) x 1(height) x 1(width)\n",
        "\n",
        "    return self.gen(x)\n",
        "\n",
        "\n",
        "def gen_noise(num, z_dim, device='cuda'):\n",
        "  return torch.randn(num, z_dim, device=device) # 128 x 200\n",
        "\n",
        "\n",
        "# n - width or height\n",
        "# nn.Conv2d: (n + 2 * pad - ks) // stride + 1\n",
        "# nn.ConvTranspose2d: (n - 1) * stride - 2 * padding + kernel_size"
      ],
      "metadata": {
        "id": "7SDTffUHP-Zl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(n - 1) * stride - 2 * padding + kernel_size\n",
        "- 1st step:\n",
        "- (1 - 1) * 1 - 2 * 0 + 4 = 4x4 image, channels: 200 to 512\n",
        "- 2nd step\n",
        "- (4 - 1) * 2 - 2 * 1 + 4 = 8x8 image, channels: 512 to 256\n",
        "- 3rd step\n",
        "- (8 - 1) * 2 - 2 * 1 + 4 = 16x16 image, channels: 256 to 128\n",
        "- 4th step\n",
        "- (16 - 1) * 2 - 2 * 1 + 4 = 32x32 image, channels: 128 to 64\n",
        "- 5th step\n",
        "- (32 - 1) * 2 - 2 * 1 + 4 = 64x64 image, channels: 64 to 32\n",
        "- 6th step\n",
        "- (64 - 1) * 2 - 2 * 1 + 4 = 128x128 image, channels: 32 to 3"
      ],
      "metadata": {
        "id": "Eg6Kn3JhY2Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# critic model\n",
        "# Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "# (n + 2 * padding - kernel_size) // stride + 1\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, d_dim=16):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.crit = nn.Sequential(\n",
        "        # 1st block\n",
        "        nn.Conv2d(in_channels=3, out_channels=d_dim, kernel_size=4, stride=2, padding=1),\n",
        "\n",
        "        # instead of batchnorm2d, normalizing according to the values of the whole instance insted of values of the batch\n",
        "        nn.InstanceNorm2d(d_dim), # works the best\n",
        "\n",
        "        # leaky keeps information, negative values have little slope(small negative numbers), but theyre not converted to 0\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # 2nd block\n",
        "        nn.Conv2d(in_channels=d_dim, out_channels=d_dim*2, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(d_dim*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # 3rd block\n",
        "        nn.Conv2d(in_channels=d_dim*2, out_channels=d_dim*4, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(d_dim*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # 4th block\n",
        "        nn.Conv2d(in_channels=d_dim*4, out_channels=d_dim*8, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(d_dim*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # 5th block\n",
        "        nn.Conv2d(in_channels=d_dim*8, out_channels=d_dim*16, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(d_dim*16),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # 6th block\n",
        "        # we return 1 value, either its fake or real\n",
        "        # stride has to be 1 and padding 0\n",
        "        nn.Conv2d(in_channels=d_dim*16, out_channels=1, kernel_size=4, stride=1, padding=0),\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, image):\n",
        "    # image: 128(batch) x 3(channels) x 128(height) x 128(width)\n",
        "    crit_pred = self.crit(image) # 128 x 1 x 1 x 1\n",
        "    return crit_pred.view(len(crit_pred), -1) # 128(batch values) x 1(fake or real)\n"
      ],
      "metadata": {
        "id": "-kkyrWSvgqsx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(n + 2 * padding - kernel_size) // stride + 1\n",
        "- 1st step\n",
        "- (128 + 2 * 1 - 4) //2 + 1 = 64x64 image, channels: 3 to 16\n",
        "- 2nd step\n",
        "- (64 + 2 * 1 - 4) //2 + 1 = 32x32 image, channels: 16 to 32\n",
        "- 3rd step\n",
        "- (32 + 2 * 1 - 4) //2 + 1 = 16x16 image, channels: 32 to 64\n",
        "- 4th step\n",
        "- (16 + 2 * 1 - 4) //2 + 1 = 8x8 image, channels: 64 to 128\n",
        "- 5th step\n",
        "- (8 + 2 * 1 - 4) //2 + 1 = 4x4 image, channels: 128 to 256\n",
        "- 6th step\n",
        "- (4 + 2 * 0 - 4) //1 + 1 = 1x1 image, channels: 256 to 1"
      ],
      "metadata": {
        "id": "PvqXR_byr5W1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative way to initialize parameters\n"
      ],
      "metadata": {
        "id": "-637eyTPwxj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "    torch.nn.init.normal(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant(m.bias, 0)\n",
        "\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "    torch.nn.init.normal(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant(m.bias, 0)\n",
        "\n",
        "# Initializations - NOT HERE\n",
        "# gen = gen.apply(init_weights)\n",
        "# crit = crit.apply(init_weights)"
      ],
      "metadata": {
        "id": "2vtp0tf-w07p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "import gdown, zipfile\n",
        "\n",
        "# url = ''\n",
        "path = '/content/drive/MyDrive'\n",
        "download_path = f'{path}/img_align_celeba.zip'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "# gdown.download(url, download_path, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(download_path, 'r') as ziphandler:\n",
        "  ziphandler.extractall('.')"
      ],
      "metadata": {
        "id": "nLFDg_nDxzO3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?resourcekey=0-dYn9z10tMJOBAkviAcfdyQ\n",
        "!unzip -q img_align_celeba.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_2zhZdZx7_X",
        "outputId": "e9e2d0f7-9401-4056-da8c-06a397cb0461"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-17 10:52:45--  https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?resourcekey=0-dYn9z10tMJOBAkviAcfdyQ\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.121.138, 108.177.121.100, 108.177.121.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.121.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘view?resourcekey=0-dYn9z10tMJOBAkviAcfdyQ’\n",
            "\n",
            "\r          view?reso     [<=>                 ]       0  --.-KB/s               \rview?resourcekey=0-     [ <=>                ]  88.03K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-06-17 10:52:45 (26.0 MB/s) - ‘view?resourcekey=0-dYn9z10tMJOBAkviAcfdyQ’ saved [90138]\n",
            "\n",
            "unzip:  cannot find or open img_align_celeba.zip, img_align_celeba.zip.zip or img_align_celeba.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Dataset(Dataset):\n",
        "class Dataset():\n",
        "  def __init__(self, path, size=128, lim=10000):\n",
        "    self.sizes = [size, size]\n",
        "\n",
        "    # paths to the images\n",
        "    items, labels = [], []\n",
        "\n",
        "    for data in os.listdir(path)[:lim]:\n",
        "      # path: './data/celeba/img_align_celeba'\n",
        "      # data: '123213.img'\n",
        "      item = os.path.join(path, data)\n",
        "      items.append(item)\n",
        "      labels.append(data)\n",
        "    self.items=items\n",
        "    self.labels=labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.items)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # open image idx\n",
        "    data = PIL.Image.open(self.items[idx]).convert('RGB') # size of the image: fe. 1278, 121\n",
        "    data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # 128 x 128 x 3\n",
        "    data = np.transpose(data, (2,0,1)).astype(np.float32, copy=False) # 3 x 128 x 128\n",
        "    # from np to tensor for training, div = standarizing\n",
        "    data = torch.from_numpy(data).div(255) # leaving values from 0 to 1\n",
        "    return data, self.labels[idx]\n"
      ],
      "metadata": {
        "id": "nCN1xe1C0tu_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "data_path = './img_align_celeba'\n",
        "ds = Dataset(data_path, size=128, lim=10000)\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(dataset=ds, batch_size=128, shuffle=True)\n",
        "\n",
        "# Models\n",
        "gen = Generator(z_dim=z_dim).to(device)\n",
        "crit = Critic().to(device)\n",
        "\n",
        "# Optimizers\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.9)) # betas - internal calculations, works well with this architecture\n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "# Initializations - OPTIONAL\n",
        "gen = gen.apply(init_weights)\n",
        "crit = crit.apply(init_weights)\n",
        "\n",
        "# wandb - OPTIONAL\n",
        "if (wandbact==1):\n",
        "  wandb.watch(gen, log_freq=100)\n",
        "  wandb.watch(crit, log_freq=100)\n",
        "\n",
        "x, y = next(iter(dataloader))\n",
        "show(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "4AZHNEEnEGIu",
        "outputId": "22044583-b8e2-46e6-faaf-6d12779fb0f7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-b0eaf9aca70a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     )\n\u001b[0;32m-> 1159\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1160\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient penalty calculation\n",
        "\n",
        "def get_gp(real, fake, crit, alpha, gamma=10): # alpha does random interpolations, gamma stands for intensity of gp regularizations\n",
        "  mix_images = real * alpha + fake * (1-alpha) # 128(batch) x 3 x 128 x 128, linear interpolation\n",
        "  mix_scores = crit(mix_images) # predictions: 128(batch) x 1\n",
        "\n",
        "  # we want to penalize gradients that are to large\n",
        "  # computing and returning the sum of the gradients of the outputs with respect to the inputs\n",
        "  gradient = torch.autograd.grad(\n",
        "      inputs = mix_images,\n",
        "      outputs = mix_scores,\n",
        "\n",
        "      # puting ones to take into account all the grades and outputs\n",
        "      grad_outputs = torch.ones_like(mix_scores),\n",
        "      retain_graph=True,\n",
        "      create_graph=True,\n",
        "\n",
        "  )[0] # return first batch 128(bs) x 3 x 128 x 128\n",
        "\n",
        "  gradient = gradient.view(len(gradient), -1) # 128 x 49512(128x128x3)\n",
        "  gradient_norm = gradient.norm(2, dim=1) # L2 norm\n",
        "  gp = gamma * ((gradient_norm - 1)**2).mean()\n",
        "\n",
        "  return gp"
      ],
      "metadata": {
        "id": "qXyHOTgiEgff"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving and loading checkpoints\n",
        "\n",
        "root_path='./data/'\n",
        "\n",
        "def save_checkpoint(name):\n",
        "  torch.save({\n",
        "      'epoch':epoch,\n",
        "      'model_state_dict':gen.state_dict(),\n",
        "      'optimizer_state_dict':gen_opt.state_dict(),\n",
        "  }, f'{root_path}G-{name}.pkl')\n",
        "\n",
        "  torch.save({\n",
        "      'epoch':epoch,\n",
        "      'model_state_dict':crit_cycles.state_dict(),\n",
        "      'optimizer_state_dict':crit_opt.state_dict(),\n",
        "  }, f'{root_path}C-{name}.pkl')\n",
        "\n",
        "  print('Saved checkpoint')\n",
        "\n",
        "\n",
        "def load_checkpoint(name):\n",
        "  # generator\n",
        "  # loading file\n",
        "  checkpoint = torch.load(f'{root_path}G-{name}.pkl')\n",
        "  # loading values to the model\n",
        "  gen.load_state_dict(checkpoint['model_state_dict'])\n",
        "  gen_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  # critic\n",
        "  checkpoint = torch.load(f'{root_path}C-{name}.pkl')\n",
        "  crit.load_state_dict(checkpoint['model_state_dict'])\n",
        "  crit_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  print('Loaded checkpoint')"
      ],
      "metadata": {
        "id": "hhk5S9DbNdNz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=1\n",
        "save_checkpoint('test')\n",
        "load_checkpoint('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "t_Gul5rjQnic",
        "outputId": "d291b248-2625-4bb5-a46f-f1225271edf2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gen' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f185b20e4e54>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-2c7b100e2996>\u001b[0m in \u001b[0;36msave_checkpoints\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      6\u001b[0m   torch.save({\n\u001b[1;32m      7\u001b[0m       \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgen_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   }, f'{root_path}G-{name}.pkl')\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for real, _ in tqdm(dataloader):\n",
        "    cur_bs = len(real) # 128\n",
        "    real = real.to(device)\n",
        "\n",
        "\n",
        "    ## Critic\n",
        "    mean_crit_loss = 0\n",
        "    for _ in range(crit_cycles):\n",
        "\n",
        "      # zeroing gradient of the optimizer\n",
        "      crit_opt.zero_grad()\n",
        "\n",
        "      noise = gen_noise(cur_bs, z_dim)\n",
        "      fake = gen(noise)\n",
        "      # detaching for not affecting the parameters of the generator\n",
        "      crit_fake_pred = crit(fake.detach())\n",
        "      crit_real_pred = crit(real)\n",
        "\n",
        "      # alpha vector (numbers size of the batch)\n",
        "      alpha = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True) # 128 x 1 x 1 x 1\n",
        "\n",
        "      # calculating gradient penalty\n",
        "      gp = get_gp(real, fake.detach(), crit, alpha)\n",
        "\n",
        "      # calculating loss\n",
        "      crit_loss = crit_fake_pred.mean() - crit_real_pred.mean() + gp\n",
        "\n",
        "      #.item - taking only the number from the tensor\n",
        "      mean_crit_loss += crit_loss.item() / crit_cycles\n",
        "\n",
        "      # optimizer backpropagation\n",
        "      crit_loss.backward(retain_graph=True)\n",
        "      crit_opt.step()\n",
        "\n",
        "    # list of losses values\n",
        "    crit_losses += [mean_crit_loss]\n",
        "\n",
        "\n",
        "    ## Generator\n",
        "\n",
        "    # zeroing gradient of the optimizer\n",
        "    gen_opt.zero_grad()\n",
        "\n",
        "    # creating noise 128 x 200\n",
        "    noise = gen.noise(cur_bs, z_dim)\n",
        "\n",
        "    # passing noise through generator\n",
        "    fake = gen(noise)\n",
        "\n",
        "    # passing them through critic\n",
        "    crit_fake_pred = crit(fake)\n",
        "\n",
        "    # negative of the pred of the critic\n",
        "    gen_loss = -crit_fake_pred.mean()\n",
        "\n",
        "    # backpropagation\n",
        "    gen_loss.backward()\n",
        "\n",
        "    # updating the parameters of the generator\n",
        "    gen_opt.step()\n",
        "\n",
        "    gen_losses+=[gen_loss.item()]\n",
        "\n",
        "    ## Statistics\n",
        "\n",
        "    if (wandb==1):\n",
        "      wandb.log(\n",
        "          {'Epoch':epoch,\n",
        "           'Step':cur_step,\n",
        "           'Critic loss':mean_crit_loss,\n",
        "           'Gen loss':gen_loss,\n",
        "           }\n",
        "      )\n",
        "\n",
        "    if cur_step % save_step == 0 and cur_step > 0:\n",
        "      print('Saving checkpoint:', cur_step, save_step)\n",
        "      # best to save the files with the different names fe. nr of epoch\n",
        "      save_checkpoint('latest')\n",
        "\n",
        "    if (cur_step % show_step == 0 and cur_step > 0):\n",
        "      show(fake, wandbactivation=1, name='fake')\n",
        "      show(real, wandbactivation=1, name='real')\n",
        "\n",
        "      gen_mean = sum(gen_losses[-show_step:]) / show_step\n",
        "      crit_mean = sum(crit_losses[-show_step:]) / show_step\n",
        "      print(f'Epoch: {epoch}, step: {cur_step}, Generator loss: {gen_loss}, Critic loss: {crit_loss}')\n",
        "\n",
        "      plt.plot(range(len(gen_losses)),\n",
        "               torch.Tensor(gen_losses),\n",
        "               label='Generator loss')\n",
        "\n",
        "      plt.plot(range(len(crit_losses)),\n",
        "               torch.Tensor(crit_losses),\n",
        "               label='Critic loss')\n",
        "\n",
        "      plt.ylim(-200,200)\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "    cur_step += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "edce130c214b40fa9667e8a7e149e747",
            "05d53c432d3745b08e371a2db3b55af1",
            "76371f7d63954d3f9988b4da973169d2",
            "45a33fd2477c4139b04274db625d31dd",
            "9e823c71052440ecb62dda39f490929d",
            "a9b73971c36041a7be7487119eac35dd",
            "7eee846b336a4b4090a300be69461c9b",
            "35182e3f88dc43998fa40bb7d2cda8f0",
            "2d2e1ac836bc4f6099b6fcc91df416fb",
            "6d8e3b2456b2499aba2158356f4f3041",
            "9fd8b21e7c854e8c80189a24d41096d4"
          ]
        },
        "id": "TjJ35qCJQ1Ss",
        "outputId": "01e6ad2c-be12-46ff-8de7-df728dee7663"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/79 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edce130c214b40fa9667e8a7e149e747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c6b16635a066>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcur_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZBYdYAouXarX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}